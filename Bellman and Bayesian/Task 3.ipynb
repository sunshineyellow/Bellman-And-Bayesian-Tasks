{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4797b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e790cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a53d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, \n",
    "            learning_rate=0.1, \n",
    "            n_steps=2048,\n",
    "            batch_size=64,\n",
    "            gae_lambda=0.95,\n",
    "            gamma=0.99,\n",
    "            n_epochs=10,\n",
    "            clip_range=0.2,\n",
    "            seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "906933b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.6     |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 902      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 9.37     |\n",
      "|    ep_rew_mean          | 9.37     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 671      |\n",
      "|    iterations           | 2        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 4096     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 11.74526 |\n",
      "|    clip_fraction        | 0.993    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0117  |\n",
      "|    explained_variance   | -0.00116 |\n",
      "|    learning_rate        | 0.1      |\n",
      "|    loss                 | 3.76     |\n",
      "|    n_updates            | 10       |\n",
      "|    policy_gradient_loss | 0.335    |\n",
      "|    value_loss           | 9.19     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 9.12      |\n",
      "|    ep_rew_mean          | 9.12      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 599       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.94e-06 |\n",
      "|    explained_variance   | 0.905     |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 0.248     |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -1.58e-08 |\n",
      "|    value_loss           | 0.689     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 9.32      |\n",
      "|    ep_rew_mean          | 9.32      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 573       |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 14        |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0.00146   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0012   |\n",
      "|    explained_variance   | 0.966     |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 0.523     |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | -0.000106 |\n",
      "|    value_loss           | 0.732     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 9.2       |\n",
      "|    ep_rew_mean          | 9.2       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 559       |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 18        |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.59e-10 |\n",
      "|    explained_variance   | 0.941     |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 0.205     |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | 1.41e-09  |\n",
      "|    value_loss           | 0.625     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x22cddc362d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d95fda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 9.6 ± 0.4898979485566356\n"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Mean reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3005be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba8d31fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: lr=0.0001, gamma=0.999, n_steps=2048, batch_size=32 -> mean_reward=430.09\n",
      "Params: lr=0.0003, gamma=0.999, n_steps=1024, batch_size=32 -> mean_reward=439.43\n",
      "Params: lr=0.0001, gamma=0.999, n_steps=2048, batch_size=128 -> mean_reward=122.41\n",
      "Params: lr=0.001, gamma=0.999, n_steps=1024, batch_size=32 -> mean_reward=500.00\n",
      "Params: lr=0.0001, gamma=0.99, n_steps=512, batch_size=32 -> mean_reward=182.81\n",
      "Best hyperparams found: (0.001, 0.999, 1024, 32) with mean reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "hyperparam_choices = {\n",
    "    \"learning_rate\": [1e-4, 3e-4, 1e-3],\n",
    "    \"gamma\": [0.95, 0.99, 0.999],\n",
    "    \"n_steps\": [512, 1024, 2048],\n",
    "    \"batch_size\": [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Generate random combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(\n",
    "    hyperparam_choices[\"learning_rate\"],\n",
    "    hyperparam_choices[\"gamma\"],\n",
    "    hyperparam_choices[\"n_steps\"],\n",
    "    hyperparam_choices[\"batch_size\"]\n",
    "))\n",
    "\n",
    "# Randomly select a subset of combinations to evaluate\n",
    "random.shuffle(param_combinations)\n",
    "param_combinations = param_combinations[:5]  # Evaluate 5 random sets\n",
    "\n",
    "best_mean_reward = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for (lr, gm, ns, bs) in param_combinations:\n",
    "    # Create and train model with these hyperparams\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0,\n",
    "                learning_rate=lr,\n",
    "                gamma=gm,\n",
    "                n_steps=ns,\n",
    "                batch_size=bs,\n",
    "                seed=0)\n",
    "    model.learn(total_timesteps=10000)\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_env = gym.make(\"CartPole-v1\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "    print(f\"Params: lr={lr}, gamma={gm}, n_steps={ns}, batch_size={bs} -> mean_reward={mean_reward:.2f}\")\n",
    "\n",
    "    # Update best\n",
    "    if mean_reward > best_mean_reward:\n",
    "        best_mean_reward = mean_reward\n",
    "        best_params = (lr, gm, ns, bs)\n",
    "\n",
    "print(\"Best hyperparams found:\", best_params, \"with mean reward:\", best_mean_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cccd0ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rewards with different seeds: [500.0, 421.13, 500.0]\n"
     ]
    }
   ],
   "source": [
    "def train_with_seeds(seeds, n_timesteps=10000):\n",
    "    rewards = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Reset the environment with a specific seed\n",
    "        env = gym.make('CartPole-v1')\n",
    "        env.reset(seed=seed)\n",
    "\n",
    "        # Train the model\n",
    "        model = PPO('MlpPolicy', env, verbose=0,learning_rate = 0.001, gamma=0.999,n_steps=1024,batch_size=32)\n",
    "        model.learn(total_timesteps=n_timesteps)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "        rewards.append(mean_reward)\n",
    "\n",
    "        env.close()  # Close the environment after training\n",
    "\n",
    "    print(f\"Mean rewards with different seeds: {rewards}\")\n",
    "\n",
    "    \n",
    "# Example of running with different seeds\n",
    "seeds = [0,1,2]\n",
    "train_with_seeds(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658b019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
